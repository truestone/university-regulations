<context>
# Overview  
The University Regulation Management and AI Search System is a comprehensive web-based platform designed to transform how university regulations are managed and accessed. Currently, Dongeui University's regulations exist as multiple large unstructured text files (following the naming pattern `regulations*.txt`, such as `regulations9-340-20250702.txt`), making it difficult for administrators to manage and for users to find relevant information.

## Source Data Format
The system processes original regulation files with the following characteristics:
- **File Pattern**: `regulations*.txt` (e.g., `regulations9-340-20250702.txt`)
- **Format**: Plain text files with Korean language content
- **Encoding**: EUC-KR or UTF-8 encoding
- **Size**: Large files (74,707+ lines) containing complete regulation sets
- **Structure**: Hierarchical regulation content with inconsistent formatting requiring specialized parsing

This system solves three critical problems:
1. **Inefficient Regulation Management**: Administrators struggle to maintain and update regulations in text files, leading to errors and inconsistencies
2. **Poor Information Accessibility**: Users cannot easily search for specific regulation information using natural language
3. **Lack of Data Structure**: The current text-based format prevents automated processing and intelligent search capabilities

The platform serves two primary user groups:
- **Administrators**: University staff responsible for managing and updating regulations
- **End Users**: Students, faculty, and staff who need to find specific regulation information

The system provides immense value by digitizing and structuring regulation management, enabling natural language AI-powered search, and ensuring data consistency and accessibility.

# Core Features  

## 1. Regulation Management System
**What it does**: Provides a web-based interface for administrators to create, read, update, and delete regulations with hierarchical structure management (Edition → Chapter → Regulation → Article → Clause).

**Why it's important**: Eliminates error-prone manual text file editing and ensures data consistency across the regulation database.

**How it works**: Uses a structured database model with proper relationships between regulation components, including support for attachments, regulation history, and status management (active/abolished).

## 2. AI-Powered Natural Language Search
**What it does**: Enables users to ask questions in natural language and receive relevant regulation information with proper citations and context.

**Why it's important**: Makes regulation information accessible to non-experts who don't know specific regulation codes or legal terminology.

**How it works**: Implements RAG (Retrieval-Augmented Generation) using vector embeddings of regulation articles, semantic search with pgvector, and AI response generation with proper source attribution.

## 3. Data Transformation and Import System
**What it does**: Converts the existing regulation files (pattern: `regulations*.txt`) into a structured database with proper hierarchical relationships through a comprehensive 6-stage process. The source files are large unstructured Korean text files (e.g., `regulations9-340-20250702.txt` with 74,707+ lines) containing complete regulation sets.

**Why it's important**: Enables the transition from legacy text-based regulation management to a modern, searchable database system. This is the most critical and complex component of the system, handling the complexity of Korean regulatory text with various formatting inconsistencies.

**How it works**: 
- **Stage 1**: Text preprocessing and structure analysis with pattern recognition for editions, chapters, regulations, articles, and clauses
- **Stage 2**: AI-based parsing using GPT-4/Claude with 95%+ accuracy target for content extraction and classification
- **Stage 3**: JSON intermediate format generation with hierarchical data structure validation
- **Stage 4**: Data validation and quality management with automated error detection and manual review interface
- **Stage 5**: Database import with batch processing, relationship mapping, and duplicate prevention
- **Stage 6**: Vector embedding generation with context enhancement and progress tracking

**Source File Requirements:**
- Accept files matching pattern `regulations*.txt` (e.g., `regulations9-340-20250702.txt`)
- Support EUC-KR and UTF-8 encoding detection and conversion
- Handle large files (70,000+ lines) with efficient memory management
- Process Korean text with specialized numbering systems (제1조, ①, 가., etc.)
- Maintain file versioning and change tracking for regulation updates

**Critical Requirements:**
- Handle complex Korean text processing with various numbering systems (제1조, ①, 가., etc.)
- Process abolished regulations marked with 【폐지】 and maintain historical references
- Manage mixed content types including regulation text, attachments, forms, and indices
- Implement comprehensive error handling for parsing failures and data inconsistencies
- Provide manual review interface for quality assurance and corrections

## 4. Conversational Interface
**What it does**: Provides a chat-like interface for users to interact with the AI system, maintaining conversation history and context.

**Why it's important**: Creates an intuitive user experience similar to modern AI assistants, making regulation queries feel natural and accessible.

**How it works**: Session-based conversations with message history, real-time responses using Hotwire Turbo, and automatic session management.

## 5. RAG System Implementation
**What it does**: Implements Retrieval-Augmented Generation for semantic search with vector embeddings and AI response generation.

**Why it's important**: Provides accurate, contextual answers to natural language queries about regulations with proper source attribution.

**How it works**:
- **Vector Embeddings**: Generate 1536-dimension embeddings for each article using OpenAI text-embedding-3-small
- **Semantic Search**: Use pgvector for cosine similarity search with configurable similarity thresholds
- **Context Enhancement**: Include regulation hierarchy (edition/chapter/regulation) in embedding context
- **Response Generation**: Use structured prompts with GPT-4/Claude for accurate, cited responses
- **Caching Strategy**: Cache embeddings, search results (30min), and AI responses (12hrs) for performance

## 6. Performance Optimization
**What it does**: Ensures system responsiveness and scalability through database optimization and caching.

**Why it's important**: Maintains sub-3-second response times for search queries and supports 50+ concurrent users.

**How it works**:
- **Database Indices**: Optimized pgvector indices for embedding search, GIN indices for text search
- **Batch Processing**: Efficient embedding generation and database operations with configurable batch sizes
- **Redis Caching**: Multi-layer caching for embeddings, search results, and regulation metadata
- **Query Optimization**: Optimized joins and pagination for hierarchical regulation browsing

# User Experience  

## User Personas
- **Regulation Administrator**: University staff member responsible for maintaining regulation accuracy and completeness
- **Student**: Undergraduate or graduate student seeking information about academic policies, procedures, or requirements
- **Faculty Member**: Professor or instructor looking for teaching-related regulations, research policies, or administrative procedures
- **Staff Member**: University employee needing information about employment policies, procedures, or institutional regulations

## Key User Flows

### Administrator Flow
1. Login with secure credentials
2. Navigate to regulation management interface
3. View hierarchical regulation structure
4. Create/edit regulations with rich text editor
5. Manage attachments and related documents
6. Trigger embedding synchronization
7. Monitor system performance and AI usage

### End User Flow
1. Access search interface (no login required)
2. Enter natural language question
3. Receive AI-generated response with source citations
4. Follow links to full regulation text
5. Continue conversation with follow-up questions
6. Access conversation history within session

## UI/UX Considerations
- **Responsive Design**: Mobile-first approach supporting all device sizes
- **Accessibility**: WCAG 2.1 AA compliance for inclusive access
- **University Branding**: Integration of Dongeui University visual identity
- **Performance**: Sub-3-second response times for search queries
- **Intuitive Navigation**: Clear information hierarchy and search functionality
</context>

<PRD>
# Technical Architecture  

## System Components
- **Web Framework**: Ruby on Rails 8.0.x with PostgreSQL and pgvector extension
- **Frontend**: Rails Views with Tailwind CSS and Hotwire Turbo for real-time updates
- **AI Integration**: Multi-provider support (OpenAI, Anthropic, Google) with local LLM options
- **Caching Layer**: Redis for session management and search result caching
- **Hosting**: Render platform with PostgreSQL database and automatic SSL

## Data Models
**Core Hierarchy Models:**
- **User**: Admin authentication (email, password_digest, name, role, last_login_at)
- **Edition**: 6 regulation volumes (number, title, description, sort_order, is_active)
- **Chapter**: Regulation chapters (edition_id, number, title, description, sort_order, is_active)
- **Regulation**: Individual regulations (chapter_id, code, title, status, abolished_at, enacted_at, department, sort_order)
- **Article**: Regulation articles (regulation_id, number, title, content, embedding, embedding_updated_at, sort_order)
- **Clause**: Article sub-items (article_id, type, number, content, sort_order)

**Supporting Models:**
- **Attachment**: Regulation appendices (regulation_id, type, number, title, content, file_path)
- **RegulationHistory**: Change tracking (regulation_id, version, amendment_date, amendment_reason, amended_by)
- **Conversation**: Chat sessions (session_id, title, created_at, expires_at, last_message_at, message_count)
- **Message**: Chat messages (conversation_id, role, content, sources, processing_time, tokens_used, created_at)
- **AiSetting**: AI configuration (provider, model_name, api_key_encrypted, monthly_budget, current_usage, is_active, environment)
- **SystemLog**: Audit trail (action, user_id, target_type, target_id, details, ip_address, created_at)

## APIs and Integrations
- **AI Provider APIs**: OpenAI, Anthropic Claude, Google Gemini for embeddings and chat completion
- **Vector Database**: pgvector for semantic search and similarity matching
- **Authentication**: Rails built-in secure password system with session management
- **File Storage**: Local file system for attachments with optional cloud storage integration

## Infrastructure Requirements
- **Local Development Environment**:
  - PostgreSQL 14+ with pgvector extension (local installation)
  - Ruby 3.2+ and Rails 8.0.x development environment
  - Minimum 4GB RAM for local development and data processing
  - 20GB storage for local database and file processing
  - Optional: Local LLM setup (Ollama, LM Studio) for API-free development
  
- **Production Environment**:
  - Render platform with PostgreSQL database and automatic SSL
  - Minimum 2GB RAM for production embedding processing
  - 10GB storage for production database, regulations, and attachments
  - Reliable internet connection for AI API calls
  - Redis instance for caching and session management

- **Environment Configuration**:
  - **Database**: Environment-specific connection settings (local PostgreSQL vs Render PostgreSQL)
  - **AI APIs**: Local LLM fallback for development, cloud APIs for production
  - **File Storage**: Local file system for development, with production storage options
  - **Caching**: Optional Redis for local development, required for production
  - **Security**: Environment-specific SSL/TLS and API key management

- **Development Tools**:
  - **Database Migration Tools**: Support for local → production data migration
  - **Environment Switching**: Easy configuration switching between local/production
  - **Performance Monitoring**: Local development metrics and production monitoring
  - **Backup and Recovery**: Local backup capabilities and production disaster recovery
  - **Version Control Strategy**: Database schema versioning through Rails migrations, avoiding binary DB files in Git
  - **Data Management**: SQL dump generation for deployment, seed data for development setup
  - **Database Synchronization**: Tools for syncing schema changes between local and production environments

# Development Roadmap  

## Development Strategy
The development follows a **local-first approach** to ensure thorough testing and validation before production deployment:

### Local Development Priority
1. **Complete local development environment** with full PostgreSQL + pgvector setup
2. **Local data processing** using actual `regulations*.txt` files for comprehensive testing
3. **Thorough validation** of all parsing and import processes in local environment
4. **Production deployment** only after local validation is complete

### Environment-Specific Configuration
- **Local Development**: Use local PostgreSQL, optional local LLM for testing, sample/full data processing
- **Production**: Render PostgreSQL, cloud AI APIs, production-ready data migration tools
- **Hybrid Support**: Same codebase supports both environments with environment-specific configurations

### Data Migration Strategy
1. **Local Database Construction**: Build complete regulation database locally with full validation
2. **Production Migration Options**:
   - **Option A**: Deploy application and re-run data import process on production
   - **Option B**: Export local database and import to production (faster, pre-validated)
   - **Option C**: Hybrid approach with incremental updates

### Version Control Strategy
- **Database Schema**: Use Rails migrations for schema version control (stored in Git)
- **Sample Data**: Store minimal seed data for development setup
- **Full Data**: Generate SQL dumps for production deployment (not stored in Git)
- **Binary Files**: Never commit PostgreSQL binary files to Git repository
- **Data Synchronization**: Use structured export/import processes for data migration

## Phase 1: Foundation and Data Migration (MVP)
**MVP Requirements:**
- **Local Development Environment**: Complete Rails 8.0.x setup with PostgreSQL + pgvector on local machine
- **Local Database Construction**: Full regulation database built and validated locally using actual `regulations*.txt` files
- User authentication system using has_secure_password (local testing ready)
- Complete database schema with all 12 core models and proper relationships
- **Comprehensive local data import system** implementing the full 6-stage transformation process:
  - File input system supporting `regulations*.txt` pattern (drag-and-drop or file browser)
  - Text preprocessing with Korean language support and encoding detection (EUC-KR/UTF-8)
  - AI-based parsing using GPT-4/Claude with 95%+ accuracy target (with local LLM fallback for testing)
  - JSON intermediate format generation with validation and error handling
  - Manual review interface for parsing corrections and quality assurance
  - Batch database import with relationship mapping and duplicate prevention
  - Progress tracking and rollback capabilities for failed imports
- **Production-ready data migration tools**: Same import system works in production environment
- Basic web interface for viewing imported regulations with hierarchical navigation
- Simple text-based search functionality (PostgreSQL full-text search, no AI yet)
- Admin authentication and basic CRUD operations for regulation management

**Local Development Focus:**
- **Complete local testing environment** with actual university regulation data
- **Full data validation** before any production deployment
- **Performance optimization** for large file processing (74,707+ lines)
- **Error handling and recovery** thoroughly tested locally
- **UI/UX validation** with real data and real user workflows

**Production Readiness:**
- **Environment configuration** supports both local and production setups
- **Data migration strategy** allows flexible deployment options
- **Monitoring and logging** ready for production deployment

**Deliverables:**
- **Complete local development environment** with Rails 8.0.x, PostgreSQL, and pgvector
- **Fully constructed local database** with all regulation data successfully imported and validated
- **Validated data import process** achieving 95%+ parsing accuracy from original `regulations*.txt` source files
- All regulation hierarchy properly structured locally (Edition → Chapter → Regulation → Article → Clause)
- **Production-ready application** that can be deployed with confidence
- Basic admin interface for regulation browsing and editing (locally tested)
- **Comprehensive data validation system** with error reporting and correction workflows
- Simple search interface using PostgreSQL text search capabilities
- **Flexible deployment options** supporting both fresh data import and database migration
- **Performance benchmarks** established from local testing with full dataset

**Critical Success Criteria:**
- **Local database completeness**: All 6 editions successfully imported with 95%+ accuracy
- **Zero data loss**: Complete data integrity maintained throughout local processing
- **Production readiness**: Application works seamlessly in both local and production environments
- **Deployment flexibility**: Support for both fresh import and database migration to production
- **Performance validation**: Local testing confirms system can handle full dataset efficiently
- **Human Validation Gates**: Each milestone approved by human developer before progression
- **Comprehensive Testing**: All functional, performance, and Korean language tests passed
- **Rollback Capability**: Ability to revert to any previous milestone if issues discovered

## Phase 2: AI Integration and Production Deployment
**Requirements:**
- **Production deployment** with validated local database and application
- **Database migration strategy** implementation (local → production)
- Vector embedding generation for all regulation articles with context enhancement
- Multi-provider AI integration (OpenAI/Anthropic/Google) with fallback options
- pgvector semantic search implementation with optimized indices
- RAG system with retrieval and response generation pipeline
- Conversational chat interface with session management
- Embedding synchronization system for content updates
- **Production monitoring** and performance optimization

**Deployment Options:**
- **Option A**: Deploy application and re-run complete data import on production
- **Option B**: Migrate pre-validated database from local to production
- **Option C**: Hybrid approach with core data migration + incremental updates

**Deliverables:**
- **Production deployment** on Render with SSL/TLS and automatic scaling
- **Successfully migrated regulation database** with all data integrity maintained
- Functional semantic search with natural language queries
- Complete vector embeddings for all regulation content (1536-dimension)
- Chat interface with real-time responses using Hotwire Turbo
- AI response generation with proper source citations and confidence scoring
- Conversation management with 7-day session expiration
- Admin interface for AI settings and embedding synchronization
- **Production monitoring dashboard** with performance metrics and alerts

**Performance Targets:**
- Sub-3-second response times for search queries
- 90%+ search relevance accuracy
- Support for 10+ concurrent conversations
- **99%+ uptime** with automated monitoring and alerting
- **Human Validation**: Each AI integration milestone tested and approved
- **Quality Assurance**: Manual testing of Korean language processing accuracy
- **Gradual Rollout**: Incremental feature deployment with user feedback loops

## Phase 3: Advanced Features and Production Optimization
**Requirements:**
- Advanced conversation management with persistent history and context
- Comprehensive admin dashboard with system monitoring and usage analytics
- Regulation status management (active/abolished/superseded) with historical tracking
- Performance optimization with multi-layer caching (Redis) and database query optimization
- Enhanced UI/UX with Dongeui University branding and responsive design
- Advanced search features with filtering, sorting, and regulation hierarchy navigation
- API usage monitoring and budget management with alerts

**Deliverables:**
- Complete admin interface for regulation lifecycle management
- Advanced search with filters (edition, chapter, status, date range)
- Performance-optimized system with caching layers and optimized database indices
- Professional UI matching university branding with accessibility compliance
- Comprehensive system monitoring dashboard with usage metrics and performance analytics
- API cost tracking and budget management system with automated alerts

## Phase 4: Production Readiness and Security Hardening
**Requirements:**
- Production deployment configuration on Render with SSL/TLS encryption
- Comprehensive security audit and penetration testing
- Performance monitoring and alerting with error tracking
- User documentation, training materials, and system administration guides
- Backup and disaster recovery procedures with automated testing
- Security hardening including API key encryption, audit logging, and access controls
- Load testing and capacity planning for 50+ concurrent users

**Deliverables:**
- Production-ready deployment on Render with automated CI/CD pipeline
- Complete security audit report and remediation of identified vulnerabilities
- Performance monitoring dashboard with real-time alerts and error tracking
- Comprehensive user guides, admin documentation, and training materials
- Tested backup and recovery procedures with documented RTO/RPO targets
- Security compliance report with Rails built-in protections and custom security measures

# Logical Dependency Chain

## Foundation First (Phase 1)
1. **Local Development Environment Setup**: Complete Rails + PostgreSQL + pgvector installation locally
2. **Database Schema**: Must be established locally before any data processing - includes all 12 models with proper relationships
3. **Data Models**: Core Rails models define the regulation hierarchy and establish data integrity constraints
4. **Local Text Processing**: Convert 74,707+ line unstructured text to structured data using 6-stage transformation in local environment
5. **Local Data Validation**: Ensure 95%+ parsing accuracy and data integrity using actual regulation files
6. **Local Web Interface**: Provide immediate visibility into imported data for validation and early feedback
7. **Production Readiness**: Prepare application for deployment with validated data and processes

## Production Deployment (Phase 2)
1. **Environment Configuration**: Set up production environment with proper database and API configurations
2. **Database Migration**: Choose and implement appropriate data migration strategy (fresh import vs. database migration)
3. **AI Integration**: Add semantic search capabilities incrementally with vector embeddings
4. **Chat Interface**: Build conversational experience on top of proven search functionality
5. **Response Generation**: Enhance search results with AI-generated answers and proper citations
6. **Production Monitoring**: Implement comprehensive monitoring and alerting for production use

## Atomic Feature Development Strategy
Each feature must be independently functional while building toward the complete system:
- **Local Development Complete**: Full functionality works locally before production deployment
- **Search without AI**: Text-based search provides core functionality immediately
- **AI without Conversation**: Semantic search valuable before conversational features
- **Conversation without History**: Single-turn interactions establish AI integration before session management
- **Administration without Monitoring**: Core CRUD operations before advanced analytics and monitoring
- **Flexible Deployment**: System supports both fresh data import and database migration for production

## Risk Mitigation through Local-First Development
1. **Thorough Local Testing**: All components tested completely in local environment
2. **Data Validation**: Complete data integrity verification before production deployment
3. **Performance Optimization**: Local benchmarking and optimization before production load
4. **Error Handling**: Comprehensive error recovery tested with real data locally
5. **Production Confidence**: Deploy only after complete local validation and testing

**Critical Dependencies:**
- **Local development environment** must be complete before starting data processing
- **Local database schema** must be established before any data import
- **Local data import** must achieve 95%+ accuracy before production deployment
- **Local search functionality** must be working before adding AI complexity
- **Production deployment** only after complete local validation and testing
- **AI integration** must be stable in local environment before production use
- **Core features** must be solid locally before production performance optimization

# Risks and Mitigations  

## Technical Challenges
**Risk**: Complex data parsing from original regulation files (`regulations*.txt` pattern) containing 74,707+ lines of unstructured Korean text with inconsistent formatting, encoding variations (EUC-KR/UTF-8), and mixed numbering systems
**Mitigation**: Implement staged parsing with AI assistance (GPT-4/Claude), comprehensive manual validation interface, automated error detection, complete rollback capabilities for failed imports, and robust encoding detection with conversion utilities

**Risk**: AI API rate limits and costs exceeding monthly budget constraints
**Mitigation**: Implement multi-layer caching strategies, efficient batch processing, real-time usage monitoring with alerts, and fallback to local LLM options (Ollama, LM Studio, GPT4All)

**Risk**: Vector search performance degradation with large regulation dataset
**Mitigation**: Optimize pgvector indices configuration, implement result caching with Redis, use appropriate similarity thresholds, and monitor query performance metrics

**Risk**: Korean text processing complexity with various numbering systems and special characters
**Mitigation**: Develop specialized Korean text processing functions, implement comprehensive pattern recognition for regulation structures, and establish manual review workflows for edge cases

## MVP Scope Definition and Feature Management
**Risk**: Feature creep preventing timely MVP delivery and project scope expansion
**Mitigation**: Strictly focus on core data import and basic search functionality for MVP, defer all advanced features to subsequent phases, establish clear acceptance criteria for each phase

**Risk**: Perfectionism in data parsing accuracy causing development delays
**Mitigation**: Accept 95% automated parsing accuracy target, implement efficient manual review interface for remaining 5%, establish clear quality thresholds and move-forward criteria

**Risk**: Over-engineering the AI integration before proving core value
**Mitigation**: Start with simple prompt-based responses, iterate based on actual user feedback, implement progressive enhancement approach to AI features

**Risk**: Insufficient testing with real regulation data leading to production issues
**Mitigation**: Use actual university regulation files (matching `regulations*.txt` pattern) throughout development, involve domain experts in validation, implement comprehensive integration testing with real data, and establish file versioning for regulation updates

## Resource Constraints and Budget Management
**Risk**: Limited development time and budget affecting project completion
**Mitigation**: Prioritize features by user impact and technical risk, use proven technologies (Rails, PostgreSQL), leverage existing libraries and frameworks, implement agile development practices

**Risk**: AI API costs exceeding allocated budget during development and operation
**Mitigation**: Implement real-time usage monitoring with automatic spending limits, provide local LLM fallback options, optimize API usage through caching and batch processing

**Risk**: Inadequate server resources on Render free tier affecting performance
**Mitigation**: Optimize application performance early, implement efficient caching strategies, plan for upgrade to paid tiers based on usage metrics, monitor resource consumption continuously

**Risk**: Single point of failure in AI provider dependencies
**Mitigation**: Implement multi-provider support (OpenAI, Anthropic, Google), provide local LLM fallback options, design provider abstraction layer for easy switching

# Appendix  

## Research Findings
- **Data Analysis**: Original regulation files following `regulations*.txt` pattern (e.g., `regulations9-340-20250702.txt`) contain 74,707+ lines with 6 editions and complex hierarchical structure, mixed content types (regulations, attachments, forms, indices), and various formatting inconsistencies requiring specialized parsing
- **File Format Analysis**: Korean text files with EUC-KR/UTF-8 encoding, inconsistent line endings, and mixed numbering systems throughout the hierarchical structure
- **User Research**: Primary need is natural language search for regulation information, with secondary needs for regulation management and maintenance workflows
- **Technical Research**: pgvector provides excellent performance for semantic search at university regulation scale, with efficient cosine similarity operations and proper indexing
- **AI Provider Analysis**: OpenAI offers best balance of cost and performance for embeddings (text-embedding-3-small) and chat completion (GPT-4), with Anthropic Claude as strong alternative

## Technical Specifications
- **Text Processing**: Support for Korean language regulation text with EUC-KR/UTF-8 encoding detection, specialized pattern recognition for Korean numbering systems (제1조, ①, 가.), and proper handling of special characters
- **Database**: PostgreSQL 14+ with pgvector extension for vector operations, optimized with specific indices for regulation hierarchy and full-text search
- **AI Models**: text-embedding-3-small (1536-dimension) for embeddings, GPT-4 or Claude-3 for response generation with temperature=0.1 for consistency
- **Performance**: Target 3-5 second response times for search queries, support for 50 concurrent users with current architecture, 20-minute maximum for full embedding synchronization
- **Scalability**: Designed for single university initially, with architecture supporting multi-university expansion

## Implementation Details
- **Parsing Strategy**: 6-stage process from text preprocessing to embedding generation with comprehensive error handling and manual review capabilities
- **Quality Metrics**: 95% automated parsing accuracy target, 90% search relevance accuracy, 99% system availability with monitoring
- **Security**: Rails built-in security features (CSRF protection, SQL injection prevention), encrypted API key storage using Rails credentials, comprehensive audit logging for all admin actions
- **Monitoring**: Real-time usage tracking with Render platform monitoring, performance metrics collection, automated error alerting via email

## Future Enhancements and Roadmap
- **Multi-university Support**: Extend system architecture to handle multiple university regulation sets with tenant isolation and shared infrastructure
- **Mobile Application**: Native mobile apps built on Rails API with offline regulation browsing and synchronized search history
- **Advanced Analytics**: Usage pattern analysis, popular query identification, content gap analysis, and user behavior insights
- **System Integration**: Connect with existing university systems (LMS, student portal, HR systems) for seamless regulation access within existing workflows
- **Advanced AI Features**: Regulation comparison tools, automated compliance checking, change impact analysis, and intelligent regulation drafting assistance

## Licensing Strategy

### Recommended License: **MIT License**
Given the project's use of permissive open-source technologies (Rails, PostgreSQL, Redis - all MIT/BSD licensed), the MIT License provides the best balance of protection and compatibility.

### License Considerations:
- **Full Compatibility**: Works seamlessly with all chosen open-source technologies
- **Commercial Flexibility**: Allows future commercialization or licensing to other universities
- **Minimal Compliance**: Simple requirements (include license text and copyright notice)
- **Clear Legal Framework**: Well-understood and court-tested license

### Open Source Dependencies:
- **Ruby on Rails**: MIT License ✅
- **PostgreSQL**: PostgreSQL License (BSD-style) ✅  
- **pgvector**: PostgreSQL License ✅
- **Redis**: BSD License ✅
- **Tailwind CSS**: MIT License ✅
- **Hotwire Turbo**: MIT License ✅

### License Compatibility Matrix:
All major dependencies use permissive licenses, ensuring no licensing conflicts or copyleft obligations.

### Alternative Considerations:
- **Apache License 2.0**: If patent protection is a concern
- **Proprietary License**: If the university requires complete code protection
- **Dual Licensing**: Open source for educational use, commercial for other universities

## AI-Assisted Development Strategy

### Development Approach
This project is designed for **AI-assisted implementation** with extensive **human validation** at each critical milestone. The development strategy prioritizes incremental progress with mandatory user testing and approval before proceeding to the next phase.

### Incremental Testing Framework
Each development phase is broken down into testable increments where the human developer can:
1. **Review and validate** each implementation step
2. **Test functionality** thoroughly before proceeding
3. **Provide feedback** for corrections or improvements
4. **Approve progression** to the next development stage

### Phase-by-Phase Testing Strategy

#### **Phase 1: Foundation and Data Migration (MVP)**

**Milestone 1.1: Local Environment Setup**
- **AI Implementation**: Set up Rails 8.0.x + PostgreSQL + pgvector locally
- **Human Testing Required**:
  - Verify Rails application starts without errors
  - Confirm PostgreSQL connection works
  - Test pgvector extension installation
  - Validate basic Rails console functionality
- **Approval Gate**: ✅ Local environment fully functional before proceeding

**Milestone 1.2: Database Schema Implementation**
- **AI Implementation**: Create all 12 core models with proper relationships
- **Human Testing Required**:
  - Review generated Rails migrations for correctness
  - Run `rails db:migrate` and verify schema creation
  - Test model relationships in Rails console
  - Validate data integrity constraints
  - Check foreign key relationships work properly
- **Approval Gate**: ✅ Database schema complete and tested before data import

**Milestone 1.3: Basic Web Interface**
- **AI Implementation**: Create basic admin interface for regulation browsing
- **Human Testing Required**:
  - Navigate through regulation hierarchy interface
  - Test CRUD operations for each model
  - Verify authentication system works
  - Test responsive design on different screen sizes
  - Validate all forms and error handling
- **Approval Gate**: ✅ Web interface functional before data processing

**Milestone 1.4: File Upload System**
- **AI Implementation**: Build file upload interface for `regulations*.txt` files
- **Human Testing Required**:
  - Upload actual `regulations9-340-20250702.txt` file
  - Verify encoding detection (EUC-KR/UTF-8) works
  - Test file validation and error messages
  - Confirm progress tracking displays correctly
- **Approval Gate**: ✅ File handling system working before parsing implementation

**Milestone 1.5: Stage 1-2 Text Preprocessing and AI Parsing**
- **AI Implementation**: Implement Korean text preprocessing and AI-based parsing
- **Human Testing Required**:
  - Process sample sections of regulation file (first 100 lines)
  - Review parsing accuracy and structure detection
  - Test pattern recognition for Korean numbering systems
  - Validate encoding conversion works correctly
  - Check error handling for parsing failures
- **Approval Gate**: ✅ Basic parsing working before full-scale processing

**Milestone 1.6: Stage 3-4 JSON Generation and Validation**
- **AI Implementation**: Build JSON intermediate format and validation system
- **Human Testing Required**:
  - Review generated JSON structure for accuracy
  - Test manual review interface for corrections
  - Validate hierarchical data structure
  - Test rollback capabilities for failed processing
  - Verify error detection and reporting
- **Approval Gate**: ✅ Data validation system working before database import

**Milestone 1.7: Stage 5 Database Import**
- **AI Implementation**: Implement batch database import with relationship mapping
- **Human Testing Required**:
  - Import processed regulation data to local database
  - Verify all relationships are correctly established
  - Test duplicate prevention mechanisms
  - Check data integrity after import
  - Navigate imported data through web interface
- **Approval Gate**: ✅ Database import successful before search implementation

**Milestone 1.8: Basic Search Functionality**
- **AI Implementation**: Implement PostgreSQL full-text search
- **Human Testing Required**:
  - Search for specific regulation terms in Korean
  - Test search accuracy and relevance
  - Verify search results display correctly
  - Test pagination and sorting
  - Validate search performance with full dataset
- **Approval Gate**: ✅ Basic search working before Phase 2

#### **Phase 2: AI Integration and Production Deployment**

**Milestone 2.1: Production Environment Setup**
- **AI Implementation**: Deploy application to Render platform
- **Human Testing Required**:
  - Access deployed application URL
  - Test database connectivity in production
  - Verify SSL/TLS encryption works
  - Test environment configuration switching
  - Validate production monitoring setup
- **Approval Gate**: ✅ Production environment ready before data migration

**Milestone 2.2: Vector Embedding Generation**
- **AI Implementation**: Implement OpenAI embedding generation for articles
- **Human Testing Required**:
  - Generate embeddings for sample articles
  - Verify embedding dimensions (1536) are correct
  - Test batch processing performance
  - Check embedding storage in database
  - Validate caching mechanisms work
- **Approval Gate**: ✅ Embeddings working before semantic search

**Milestone 2.3: Semantic Search Implementation**
- **AI Implementation**: Build pgvector semantic search with cosine similarity
- **Human Testing Required**:
  - Test natural language queries in Korean
  - Compare search results with basic text search
  - Verify similarity thresholds work correctly
  - Test search performance and response times
  - Validate result ranking and relevance
- **Approval Gate**: ✅ Semantic search functional before RAG system

**Milestone 2.4: Conversational Interface**
- **AI Implementation**: Create chat interface with Hotwire Turbo
- **Human Testing Required**:
  - Test real-time chat functionality
  - Verify conversation history works
  - Test session management (7-day expiration)
  - Check UI responsiveness and user experience
  - Validate conversation persistence
- **Approval Gate**: ✅ Chat interface working before AI responses

**Milestone 2.5: RAG System and AI Response Generation**
- **AI Implementation**: Implement GPT-4/Claude response generation with citations
- **Human Testing Required**:
  - Ask complex questions about university regulations
  - Verify AI responses are accurate and well-cited
  - Test source attribution and links to original text
  - Check response quality and Korean language handling
  - Validate confidence scoring and fallback mechanisms
- **Approval Gate**: ✅ Complete AI system functional before optimization

### Testing Requirements for AI Implementation

#### **Mandatory Test Cases**
Each milestone must pass these test categories:
1. **Functional Testing**: All features work as specified
2. **Data Integrity Testing**: No data corruption or loss
3. **Performance Testing**: Response times meet targets
4. **Error Handling Testing**: Graceful failure and recovery
5. **User Experience Testing**: Interface is intuitive and responsive

#### **Korean Language Specific Testing**
- **Encoding Handling**: EUC-KR and UTF-8 files process correctly
- **Text Processing**: Korean numbering systems (제1조, ①, 가.) are recognized
- **Search Accuracy**: Korean language queries return relevant results
- **AI Responses**: Generated responses are coherent in Korean

#### **Approval Workflow**
1. **AI Implementation**: Complete milestone implementation
2. **Automated Testing**: Run test suite (if available)
3. **Human Review**: Manual testing by developer
4. **Feedback Loop**: Corrections and improvements if needed
5. **Approval**: Explicit go/no-go decision before next milestone
6. **Documentation**: Update progress and lessons learned

### Rollback and Recovery Strategy
- **Version Control**: Each milestone tagged in Git for easy rollback
- **Database Backups**: Automatic backups before major changes
- **Configuration Management**: Environment-specific settings preserved
- **Documentation**: Detailed logs of all changes and decisions

This approach ensures that AI implementation proceeds safely with human oversight at every critical juncture, allowing for thorough testing and validation before advancing to more complex features.
</PRD>
